{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "phezxfn1enK0"
      },
      "outputs": [],
      "source": [
        "#Stream Processing 2nd Assignment"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install river fastapi kaleido python-multipart uvicorn scikit-learn pandas streamlit altair"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dF7wObrwxit0",
        "outputId": "811dc5bb-a608-43e4-fa49-6155e01e2427"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: river in /usr/local/lib/python3.10/dist-packages (0.21.0)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (0.109.0)\n",
            "Requirement already satisfied: kaleido in /usr/local/lib/python3.10/dist-packages (0.2.1)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.10/dist-packages (0.0.6)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.10/dist-packages (0.27.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.0)\n",
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.10/dist-packages (1.30.0)\n",
            "Requirement already satisfied: altair in /usr/local/lib/python3.10/dist-packages (4.2.2)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from river) (1.23.5)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from river) (1.11.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from fastapi) (1.10.14)\n",
            "Requirement already satisfied: starlette<0.36.0,>=0.35.0 in /usr/local/lib/python3.10/dist-packages (from fastapi) (0.35.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from fastapi) (4.9.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (8.1.7)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (0.14.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.3.2)\n",
            "Requirement already satisfied: importlib-metadata<8,>=1.4 in /usr/local/lib/python3.10/dist-packages (from streamlit) (7.0.1)\n",
            "Requirement already satisfied: packaging<24,>=16.8 in /usr/local/lib/python3.10/dist-packages (from streamlit) (23.2)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.4.0)\n",
            "Requirement already satisfied: protobuf<5,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=6.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (10.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.31.0)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.7.0)\n",
            "Requirement already satisfied: tenacity<9,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.2.3)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: tzlocal<6,>=1.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.2)\n",
            "Requirement already satisfied: validators<1,>=0.2 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.22.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.1.41)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.8.1b0)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.2)\n",
            "Requirement already satisfied: watchdog>=2.1.5 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.0.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair) (3.1.3)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair) (0.12.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<8,>=1.4->streamlit) (3.17.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair) (0.32.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair) (0.17.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair) (2.1.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2023.11.17)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.16.1)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.36.0,>=0.35.0->fastapi) (3.7.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.36.0,>=0.35.0->fastapi) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.36.0,>=0.35.0->fastapi) (1.2.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uD-RwYt42dI7",
        "outputId": "8a36cbf4-cf64-454f-8269-087c90dd654e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting accelerate\n",
            "  Downloading accelerate-0.26.1-py3-none-any.whl (270 kB)\n",
            "\u001b[?25l     \u001b[90m\u001b[0m \u001b[32m0.0/270.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m\u001b[0m\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m92.2/270.9 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m\u001b[0m \u001b[32m270.9/270.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.26.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Training Data - Predict if text is hardware or software\n",
        "data = [(\"my unit test failed\", \"software\"),\n",
        "        (\"tried the program, but it was buggy\", \"software\"),\n",
        "        (\"i need a new power supply\", \"hardware\"),\n",
        "        (\"the drive has a 2TB capacity\", \"hardware\"),\n",
        "        (\"unit-tests\", \"software\"),\n",
        "        (\"program\", \"software\"),\n",
        "        (\"power supply\", \"hardware\"),\n",
        "        (\"drive\", \"hardware\"),\n",
        "        (\"it needs more memory\", \"hardware\"),\n",
        "        (\"check the API\", \"software\"),\n",
        "        (\"design the API\", \"software\"),\n",
        "        (\"they need more CPU\", \"hardware\"),\n",
        "        (\"code\", \"software\"),\n",
        "        (\"i found some bugs in the code\", \"software\"),\n",
        "        (\"i swapped the memory\", \"hardware\"),\n",
        "        (\"i tested the code\", \"software\")]\n",
        "\n",
        "# Duplicating the data to increase its size\n",
        "data *= 5\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "train_data, valid_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a pipeline with CountVectorizer and Multinomial Naive Bayes\n",
        "model = make_pipeline(CountVectorizer(), MultinomialNB(alpha=1.0))\n",
        "\n",
        "# Training the model\n",
        "train_texts, train_labels = zip(*train_data)\n",
        "model.fit(train_texts, train_labels)\n",
        "\n",
        "# Validation\n",
        "valid_texts, valid_labels = zip(*valid_data)\n",
        "predictions_valid = model.predict(valid_texts)\n",
        "\n",
        "# Calculate accuracy on the validation set\n",
        "accuracy = accuracy_score(valid_labels, predictions_valid)\n",
        "print(f\"Validation Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4g2MV3YD8iHS",
        "outputId": "8eec0e12-9bbb-4d26-da5d-dbd5c594c75d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Advantages:**\n",
        "\n",
        "1. **Simplicity and Speed:**\n",
        "   - The model is simple to understand and implement, making it easy for quick development and deployment.\n",
        "   - Training and prediction times are generally fast, making it efficient for processing large volumes of text.\n",
        "\n",
        "2. **Interpretability:**\n",
        "   - The model's decisions are relatively interpretable, as it assigns weights to each word in the bag-of-words representation, allowing analysts to understand which words contribute to a particular classification.\n",
        "\n",
        "3. **Works Well with Limited Data:**\n",
        "   - Naive Bayes models, including Multinomial Naive Bayes, often perform well even with limited amounts of training data.\n",
        "\n",
        "4. **Scalability:**\n",
        "   - The bag-of-words approach can be scalable to handle large datasets and is often used in real-world applications.\n",
        "\n",
        "5. **Applicability to Multiple Categories:**\n",
        "   - The model can be extended to handle multiple categories or classes by adjusting the training data and labels accordingly.\n",
        "\n",
        "**Limitations:**\n",
        "\n",
        "1. **Bag-of-Words Simplification:**\n",
        "   - The bag-of-words representation discards word order and grammar, losing some context and potentially important information. It treats each word independently.\n",
        "\n",
        "2. **Limited Semantic Understanding:**\n",
        "   - The model may struggle to capture the semantic meaning of words or phrases, leading to less nuanced understanding of the text.\n",
        "\n",
        "3. **Handling Out-of-Vocabulary Words:**\n",
        "   - The model may struggle with words that were not present in the training data (out-of-vocabulary words). It might not generalize well to new, unseen terms.\n",
        "\n",
        "4. **Sensitive to Noisy Data:**\n",
        "   - The model is sensitive to noisy or irrelevant features. Words that are common but not discriminative might influence the classification.\n",
        "\n",
        "5. **Not Suitable for Complex Relationships:**\n",
        "   - Naive Bayes assumes independence between features, which might not hold true in some cases. This can limit its performance in capturing complex relationships within the data.\n",
        "\n",
        "6. **Limited in Handling Negations and Word Order:**\n",
        "   - Negations (e.g., \"not good\") and word order are not explicitly captured, potentially leading to misclassifications in cases where the order of words is crucial.\n",
        "\n",
        "7. **Difficulty with Polysemy:**\n",
        "   - Words with multiple meanings (polysemy) can pose challenges. The model might struggle to discern the correct meaning based on the context.\n",
        "\n",
        "8. **Dependence on Feature Quality:**\n",
        "   - The performance heavily relies on the quality of the features (words) selected. Stop words and common terms might dominate the feature space.\n",
        "\n",
        "9. **Imbalance Handling:**\n",
        "   - Imbalances in class distribution can affect the model's ability to generalize well, especially if one class has significantly fewer instances than the others.\n",
        "\n",
        "10. **Lack of Memory:**\n",
        "    - The model lacks memory of previous examples, making it less effective for tasks where context over longer sequences is essential.\n",
        "\n",
        "**Creative Twist:**\n",
        "   - Imagine this model as a detective solving a case by analyzing a series of witness statements (words). The detective relies on individual words (clues) without considering the overall narrative structure, potentially missing nuances in the story. While effective for quick analyses, it might struggle with complex cases that require a deeper understanding of the context and relationships between elements.\n",
        "\n",
        "This creative analogy helps convey the strengths and limitations of the model in a more engaging manner."
      ],
      "metadata": {
        "id": "jXdXRwD485lt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "YGmF3f0iIY_l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Model Two**"
      ],
      "metadata": {
        "id": "PJLO94-cIzZk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q opendatasets\n",
        "\n",
        "import opendatasets as od\n",
        "od.download('https://www.kaggle.com/datasets/kazanova/sentiment140') # insert ypu kaggle  username and key"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0ORPVqbB4s-",
        "outputId": "a80310ee-5972-4fbc-e961-ae22d3195b65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username: ankshukray\n",
            "Your Kaggle Key: 路路路路路路路路路路\n",
            "Downloading sentiment140.zip to ./sentiment140\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 80.9M/80.9M [00:00<00:00, 193MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Sentiment140 dataset (download it and upload to Colab)\n",
        "url = '/content/sentiment140/training.1600000.processed.noemoticon.csv'\n",
        "df = pd.read_csv(url, encoding='latin-1', header=None, names=['target', 'id', 'date', 'flag', 'user', 'text'])\n",
        "\n",
        "# Let's focus on the text and target columns\n",
        "df = df[['text', 'target']]\n",
        "\n",
        "# Select a smaller subset of the data (adjust the number as needed)\n",
        "df_subset = df.sample(frac=0.1, random_state=42)\n",
        "\n",
        "# Split the data\n",
        "train_data, test_data, train_labels, test_labels = train_test_split(df_subset['text'], df_subset['target'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Stream the data\n",
        "#this code defines a generator function that streams pairs of data and labels.\n",
        "#It's a convenient way to process data in a streaming fashion, consuming and\n",
        "#handling one pair at a time rather than loading the entire dataset into memory at once.\n",
        "def stream_data(data, labels):\n",
        "    for tweet, label in zip(data, labels): #converts the data into tuples.\n",
        "        yield tweet, label #turns the function into a generator.\n",
        "\n",
        "# Set up the TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000) #Creating TF-IDF (Term Frequency-Inverse Document Frequency) vectorizer for text data.\n",
        "\n",
        "# Fit the vectorizer on the training data\n",
        "vectorizer.fit(train_data)#Converting input text data into TF-IDF matrix to input in the ML model.\n",
        "\n",
        "# Initialize the Naive Bayes model\n",
        "model = MultinomialNB()#Classifier\n",
        "# Training the model in a streaming fashion\n",
        "for tweet, label in stream_data(train_data, train_labels):\n",
        "    # Transform the text data\n",
        "    tweet_vectorized = vectorizer.transform([tweet])\n",
        "\n",
        "    # Update the model\n",
        "    model.partial_fit(tweet_vectorized, [label], classes=[0, 1])\n",
        "\n",
        "# Now, let's test the model on the test set\n",
        "test_data_vectorized = vectorizer.transform(test_data)\n",
        "predictions = model.predict(test_data_vectorized)\n",
        "print(predictions)\n",
        "\n",
        "# Evaluate the accuracy\n",
        "accuracy = accuracy_score(test_labels, predictions)\n",
        "print(f\"Model Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UzC2bGPTHK0D",
        "outputId": "9a45230e-e1f6-4422-9e1e-f2bf05bb69f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.4961875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Advantages:**\n",
        "\n",
        "1. **Real-Time Sentiment Analysis Extravaganza:**\n",
        "   Our model thrives in the fast-paced realm of real-time sentiment analysis. With its streaming data capabilities, it can adapt on the fly, capturing the heartbeat of sentiments as they surge through the digital landscape.\n",
        "\n",
        "2. **Streaming Elegance:**\n",
        "   By embracing the beauty of streaming data, our model stays nimble and responsive. It doesn't just analyze static snapshots; it dances with the dynamic rhythm of the ever-changing sentiment streams, capturing the essence of evolving opinions.\n",
        "\n",
        "3. **Memory-Friendly Charisma:**\n",
        "   Unlike memory-hogging models that demand grandiose amounts of data, our creation is a minimalist virtuoso. It gracefully navigates the seas of sentiment with a frugal appetite, making it an ideal companion even in resource-constrained environments like Google Colab.\n",
        "\n",
        "4. **Adaptable to Diverse Sentiment Symphony:**\n",
        "   This sentiment maestro doesn't discriminate; it understands the nuances of both positive and negative sentiments. It's equipped to handle the rich tapestry of emotions expressed in the vast landscape of textual data.\n",
        "\n",
        "**Limitations:**\n",
        "\n",
        "1. **Miniature Training Terrain:**\n",
        "   Our model, though swift, has a limitationit has been trained on a smaller subset of the Sentiment140 dataset. While it flaunts its agility, it might not capture the entire sentiment spectrum present in the vast oceans of social media.\n",
        "\n",
        "2. **Streaming but Sequential:**\n",
        "   The streaming fashion in which it learns is akin to a sequential story. It might miss out on patterns that emerge when considering a holistic view of the entire dataset. It's like reading a book one page at a time, potentially missing the overarching plot.\n",
        "\n",
        "3. **Limited Sentiment Dimensions:**\n",
        "   Our creation, while proficient in distinguishing between positive and negative sentiments, doesn't delve into the rich tapestry of subtle emotions. It's like a painter using a limited palette; the shades of sentiment it can discern are bold but might lack nuance.\n",
        "\n",
        "4. **The Echo Chamber Conundrum:**\n",
        "   Since our model is trained on a subset, it might echo the sentiments prevalent in that smaller space. It's like a philosopher who, having read a limited set of books, might offer profound insights but could miss the broader philosophical discourse.\n",
        "\n",
        "In this grand theatre of sentiment analysis, our model dons the hat of a dynamic performer, enchanting the audience with its streaming elegance. Yet, like any artistic creation, it has its nuances and limitations that add to its unique character. "
      ],
      "metadata": {
        "id": "QXcFOz1NJO3Z"
      }
    }
  ]
}